{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red>SMU DS 7331 DATA MINING - LAB 2 CLASSIFICATION</font>\n",
    "\n",
    "**Team Members:**\n",
    "- YuMei Bennett\n",
    "- Liang Huang\n",
    "- Ganesh Kodi\n",
    "- Eric McCandless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue>DATA PREPARATION PART 1</font>\n",
    "\n",
    "**Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial prep of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>hours_week</th>\n",
       "      <th>country_orig</th>\n",
       "      <th>income</th>\n",
       "      <th>emp_ Federal-gov</th>\n",
       "      <th>emp_ Local-gov</th>\n",
       "      <th>emp_ Never-worked</th>\n",
       "      <th>emp_ Other_cat</th>\n",
       "      <th>emp_ Private</th>\n",
       "      <th>emp_ Self-emp-inc</th>\n",
       "      <th>...</th>\n",
       "      <th>occu_ Handlers-cleaners</th>\n",
       "      <th>occu_ Machine-op-inspct</th>\n",
       "      <th>occu_ Priv-house-serv</th>\n",
       "      <th>occu_ Prof-specialty</th>\n",
       "      <th>occu_ Protective-serv</th>\n",
       "      <th>occu_ Sales</th>\n",
       "      <th>occu_ Tech-support</th>\n",
       "      <th>occu_ Transport-moving</th>\n",
       "      <th>occu_ Other</th>\n",
       "      <th>cap_gain-loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>37</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>49</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>52</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>42</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  hours_week  country_orig  income  emp_ Federal-gov  emp_ Local-gov  \\\n",
       "0   39          40             1       0                 0               0   \n",
       "1   50          13             1       0                 0               0   \n",
       "2   38          40             1       0                 0               0   \n",
       "3   53          40             1       0                 0               0   \n",
       "4   28          40             0       0                 0               0   \n",
       "5   37          40             1       0                 0               0   \n",
       "6   49          16             0       0                 0               0   \n",
       "7   52          45             1       1                 0               0   \n",
       "8   31          50             1       1                 0               0   \n",
       "9   42          40             1       1                 0               0   \n",
       "\n",
       "   emp_ Never-worked  emp_ Other_cat  emp_ Private  emp_ Self-emp-inc  ...  \\\n",
       "0                  0               0             0                  0  ...   \n",
       "1                  0               0             0                  0  ...   \n",
       "2                  0               0             1                  0  ...   \n",
       "3                  0               0             1                  0  ...   \n",
       "4                  0               0             1                  0  ...   \n",
       "5                  0               0             1                  0  ...   \n",
       "6                  0               0             1                  0  ...   \n",
       "7                  0               0             0                  0  ...   \n",
       "8                  0               0             1                  0  ...   \n",
       "9                  0               0             1                  0  ...   \n",
       "\n",
       "   occu_ Handlers-cleaners  occu_ Machine-op-inspct  occu_ Priv-house-serv  \\\n",
       "0                        0                        0                      0   \n",
       "1                        0                        0                      0   \n",
       "2                        1                        0                      0   \n",
       "3                        1                        0                      0   \n",
       "4                        0                        0                      0   \n",
       "5                        0                        0                      0   \n",
       "6                        0                        0                      0   \n",
       "7                        0                        0                      0   \n",
       "8                        0                        0                      0   \n",
       "9                        0                        0                      0   \n",
       "\n",
       "   occu_ Prof-specialty  occu_ Protective-serv  occu_ Sales  \\\n",
       "0                     0                      0            0   \n",
       "1                     0                      0            0   \n",
       "2                     0                      0            0   \n",
       "3                     0                      0            0   \n",
       "4                     1                      0            0   \n",
       "5                     0                      0            0   \n",
       "6                     0                      0            0   \n",
       "7                     0                      0            0   \n",
       "8                     1                      0            0   \n",
       "9                     0                      0            0   \n",
       "\n",
       "   occu_ Tech-support  occu_ Transport-moving  occu_ Other  cap_gain-loss  \n",
       "0                   0                       0            0           2174  \n",
       "1                   0                       0            0              0  \n",
       "2                   0                       0            0              0  \n",
       "3                   0                       0            0              0  \n",
       "4                   0                       0            0              0  \n",
       "5                   0                       0            0              0  \n",
       "6                   0                       0            1              0  \n",
       "7                   0                       0            0              0  \n",
       "8                   0                       0            0          14084  \n",
       "9                   0                       0            0           5178  \n",
       "\n",
       "[10 rows x 50 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import all necessary modules\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Imputer\n",
    "import seaborn as sns\n",
    "\n",
    "#Read in dataset.\n",
    "col_names = ['age', 'employ_type', 'pop_num', 'edu_level', 'edu_years', 'marital', 'occ', 'relation', 'race', 'gender', 'cap_gain', 'cap_loss', 'hours_week', 'country_orig', 'income']\n",
    "df = pd.read_csv('adult.csv', names=col_names, header=None)\n",
    "\n",
    "#Replace \"?\" with \"Other_cat\"\n",
    "df['employ_type'] = df['employ_type'].str.replace('?','Other_cat')\n",
    "df['occ'] = df['occ'].str.replace('?','Other_cat')\n",
    "df['country_orig'] = df['country_orig'].str.replace('?','Other_cat')\n",
    "\n",
    "# Binary encoding of the target variable\n",
    "df['income'] = df['income'].apply(lambda inc: 0 if inc ==\" <=50K\" else 1) \n",
    "\n",
    "#Transform employ_type into multiple columns with 0 and 1\n",
    "# use pd.concat to join the new columns with original dataframe then drop the original 'employ_type' column (don't need it anymore)\n",
    "df = pd.concat([df,pd.get_dummies(df['employ_type'], prefix='emp')],axis=1)\n",
    "df.drop(['employ_type'],axis=1, inplace=True)\n",
    "\n",
    "#Transform gender into multiple columns with 0 and 1\n",
    "# use pd.concat to join the new columns with original dataframe then drop the original 'gender' column (don't need it anymore)\n",
    "df = pd.concat([df,pd.get_dummies(df['gender'], prefix='gen')],axis=1)\n",
    "df.drop(['gender'],axis=1, inplace=True)\n",
    "\n",
    "#Transform race into multiple columns with 0 and 1\n",
    "# use pd.concat to join the new columns with original dataframe then drop the original 'race' column (don't need it anymore)\n",
    "df = pd.concat([df,pd.get_dummies(df['race'], prefix='rac')],axis=1)\n",
    "df.drop(['race'],axis=1, inplace=True)\n",
    "\n",
    "#Transform education_level into multiple columns with 0 and 1\n",
    "# use pd.concat to join the new columns with original dataframe then drop the original 'edu_level' column (don't need it anymore)\n",
    "df = pd.concat([df,pd.get_dummies(df['edu_level'], prefix='edu')],axis=1)\n",
    "df.drop(['edu_level'],axis=1, inplace=True)\n",
    "\n",
    "#Consolidate education levels because many of them have the similar impact to target income.\n",
    "df['edu_ SomeCollege'] = df['edu_ Some-college'] + df['edu_ Assoc-acdm'] + df['edu_ Assoc-voc'] \n",
    "df['<HS'] = df['edu_ 12th'] + df['edu_ 11th'] + df['edu_ 10th'] + df['edu_ 9th'] + df['edu_ 7th-8th'] + df['edu_ 5th-6th']+ df['edu_ 1st-4th'] + df['edu_ Preschool'] \n",
    "df=df.drop(['edu_ Some-college','edu_ Assoc-acdm','edu_ Assoc-voc', 'edu_ 12th', 'edu_ 11th','edu_ 10th','edu_ 9th','edu_ 7th-8th','edu_ 7th-8th','edu_ 5th-6th','edu_ 1st-4th','edu_ Preschool'], 1)\n",
    "\n",
    "# drop edu_years as it is highly correlated with edu_level.\n",
    "df=df.drop(['edu_years'], 1)\n",
    "\n",
    "#Transform relation into multiple columns with 0 and 1\n",
    "# use pd.concat to join the new columns with original dataframe then drop the original 'relation' column (don't need it anymore)\n",
    "df = pd.concat([df,pd.get_dummies(df['relation'], prefix='rel')],axis=1)\n",
    "df.drop(['relation'],axis=1, inplace=True)\n",
    "\n",
    "#Transform marital into multiple columns with 0 and 1\n",
    "# use pd.concat to join the new columns with original dataframe then drop the original 'marital' column (don't need it anymore)\n",
    "df = pd.concat([df,pd.get_dummies(df['marital'], prefix='mar')],axis=1)\n",
    "df.drop(['marital'],axis=1, inplace=True)\n",
    "\n",
    "#Consolidate marital status because too many similar categories.  Married-civ-spouse and Married-AF-spouse are similar as are non-married.\n",
    "df['Married'] = df['mar_ Married-civ-spouse'] + df['mar_ Married-AF-spouse'] \n",
    "df['Sep_Div_Absent_Wid'] = df['mar_ Divorced'] + df['mar_ Separated'] + df['mar_ Widowed'] + df['mar_ Married-spouse-absent']\n",
    "df['Never_Married'] = df['mar_ Never-married']\n",
    "df=df.drop(['mar_ Married-civ-spouse','mar_ Married-AF-spouse','mar_ Divorced', 'mar_ Separated', 'mar_ Widowed','mar_ Married-spouse-absent','mar_ Never-married'], 1)\n",
    "\n",
    "#Transform occ into multiple columns with 0 and 1\n",
    "# use pd.concat to join the new columns with original dataframe then drop the original 'occ' column (don't need it anymore)\n",
    "df = pd.concat([df,pd.get_dummies(df['occ'], prefix='occu')],axis=1)\n",
    "df.drop(['occ'],axis=1, inplace=True)\n",
    "\n",
    "#Consolidate occupation by combining 'Other-service', 'Other_cat', and 'Armed-Forces. 'Other' categories are combined because they are not defined and Armed-Forces has an extremely small number of occurences.\n",
    "df['occu_ Other'] = df['occu_ Other-service'] + df['occu_ Other_cat'] + df['occu_ Armed-Forces'] \n",
    "df=df.drop(['occu_ Other-service','occu_ Other_cat','occu_ Armed-Forces'], 1)\n",
    "\n",
    "# drop pop_num as population number is an assigned index number, it has no meaning or contribution to our target income.\n",
    "df=df.drop(['pop_num'], 1)\n",
    "\n",
    "# Combine all non-U.S. native countries as only ~10% people are not from US - code native country into binary 1=United-States\n",
    "df['country_orig'] = df['country_orig'].apply(lambda inc: 1 if inc ==\" United-States\" else 0) \n",
    "\n",
    "# merge capital gain and capital losscap_gain and cap_loss as it can be mathmatically concatenated into a single feature cap_gain_loss = cap_gain - cap_loss.\n",
    "df['cap_gain-loss'] = df['cap_gain'] - df['cap_loss'] \n",
    "df=df.drop(['cap_gain','cap_loss'], 1)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue>DATA PREPARATION PART 2</font>\n",
    "\n",
    "**Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32561 entries, 0 to 32560\n",
      "Data columns (total 50 columns):\n",
      "age                        32561 non-null int64\n",
      "hours_week                 32561 non-null int64\n",
      "country_orig               32561 non-null int64\n",
      "income                     32561 non-null int64\n",
      "emp_ Federal-gov           32561 non-null uint8\n",
      "emp_ Local-gov             32561 non-null uint8\n",
      "emp_ Never-worked          32561 non-null uint8\n",
      "emp_ Other_cat             32561 non-null uint8\n",
      "emp_ Private               32561 non-null uint8\n",
      "emp_ Self-emp-inc          32561 non-null uint8\n",
      "emp_ Self-emp-not-inc      32561 non-null uint8\n",
      "emp_ State-gov             32561 non-null uint8\n",
      "emp_ Without-pay           32561 non-null uint8\n",
      "gen_ Female                32561 non-null uint8\n",
      "gen_ Male                  32561 non-null uint8\n",
      "rac_ Amer-Indian-Eskimo    32561 non-null uint8\n",
      "rac_ Asian-Pac-Islander    32561 non-null uint8\n",
      "rac_ Black                 32561 non-null uint8\n",
      "rac_ Other                 32561 non-null uint8\n",
      "rac_ White                 32561 non-null uint8\n",
      "edu_ Bachelors             32561 non-null uint8\n",
      "edu_ Doctorate             32561 non-null uint8\n",
      "edu_ HS-grad               32561 non-null uint8\n",
      "edu_ Masters               32561 non-null uint8\n",
      "edu_ Prof-school           32561 non-null uint8\n",
      "edu_ SomeCollege           32561 non-null uint8\n",
      "<HS                        32561 non-null uint8\n",
      "rel_ Husband               32561 non-null uint8\n",
      "rel_ Not-in-family         32561 non-null uint8\n",
      "rel_ Other-relative        32561 non-null uint8\n",
      "rel_ Own-child             32561 non-null uint8\n",
      "rel_ Unmarried             32561 non-null uint8\n",
      "rel_ Wife                  32561 non-null uint8\n",
      "Married                    32561 non-null uint8\n",
      "Sep_Div_Absent_Wid         32561 non-null uint8\n",
      "Never_Married              32561 non-null uint8\n",
      "occu_ Adm-clerical         32561 non-null uint8\n",
      "occu_ Craft-repair         32561 non-null uint8\n",
      "occu_ Exec-managerial      32561 non-null uint8\n",
      "occu_ Farming-fishing      32561 non-null uint8\n",
      "occu_ Handlers-cleaners    32561 non-null uint8\n",
      "occu_ Machine-op-inspct    32561 non-null uint8\n",
      "occu_ Priv-house-serv      32561 non-null uint8\n",
      "occu_ Prof-specialty       32561 non-null uint8\n",
      "occu_ Protective-serv      32561 non-null uint8\n",
      "occu_ Sales                32561 non-null uint8\n",
      "occu_ Tech-support         32561 non-null uint8\n",
      "occu_ Transport-moving     32561 non-null uint8\n",
      "occu_ Other                32561 non-null uint8\n",
      "cap_gain-loss              32561 non-null int64\n",
      "dtypes: int64(5), uint8(45)\n",
      "memory usage: 2.6 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7841\n"
     ]
    }
   ],
   "source": [
    "num_high_income = sum(df['income']!=0)\n",
    "print (num_high_income)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7946678541813826\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import count_nonzero\n",
    "\n",
    "# calculate sparsity\n",
    "sparsity = 1.0 - ( count_nonzero(df) / float(df.size) )\n",
    "print(sparsity)\n",
    "# only 21% none zero values in dataset, pretty sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (32561, 49)\n",
      "target shape: (32561,)\n"
     ]
    }
   ],
   "source": [
    "#Separate target vs. data into two different data frame\n",
    "ds=df\n",
    "ds.target=ds['income']\n",
    "ds.data=ds\n",
    "del ds.data['income']\n",
    "ds.data=ds.data.values\n",
    "\n",
    "# this holds the continuous feature data (which is tfidf)\n",
    "print ('features shape:', ds.data.shape) # there are ~11000 instances and ~130k features per instance\n",
    "print ('target shape:', ds.target.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue>MODELING AND EVALUATION 1</font>\n",
    "\n",
    "**Choose and explain your evaluation metrics that you will use (i.e., accuracy, precision, recall, F-measure, or any metric we have discussed). Why are the measure(s) appropriate for analyzing the results of your modeling? Give a detailed explanation backing up any assertions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We would like to gauge the model effectiveness by F measure. This is because \n",
    "# one, the dataset is inbanlanced, only ~23% instance map to target value of 1 (>50k)\n",
    "# two, F measure is a combination of precision and recall, target \"income\" really has \n",
    "# no benefit to bias one way or the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "accuracy 0.8466310423192679\n",
      "f1 score 0.6323074657635105\n",
      "precision score 0.7480836236933798\n",
      "recall score 0.5475643968375414\n",
      "confusion matrix\n",
      " [[11637   723]\n",
      " [ 1774  2147]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.8475523616485474\n",
      "f1 score 0.6283318358790058\n",
      "precision score 0.7609720710917665\n",
      "recall score 0.535067584799796\n",
      "confusion matrix\n",
      " [[11701   659]\n",
      " [ 1823  2098]]\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "\n",
    "#cv = StratifiedShuffleSplit(ds.target, n_iter = 1, test_size = 0.5, train_size=0.5)\n",
    "data=ds.data\n",
    "labels=ds.target\n",
    "sss = StratifiedShuffleSplit(n_splits=2,test_size = 0.5, train_size=0.5)\n",
    "iter_num=0\n",
    "for train_index, test_index in sss.split(data, labels):\n",
    "    x_train, x_test = data[train_index], data[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "    # train the reusable KNN classifier on the training data       \n",
    "    clf = KNeighborsClassifier(n_neighbors=8, weights='uniform', metric='euclidean')\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_hat = clf.predict(x_test) \n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = accuracy_score(y_test,y_hat)\n",
    "    precision = precision_score(y_test, y_hat)\n",
    "    recall = recall_score(y_test, y_hat)\n",
    "    f1=f1_score(y_test, y_hat)\n",
    "    conf = confusion_matrix(y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"f1 score\", f1 )\n",
    "    print(\"precision score\", precision )\n",
    "    print(\"recall score\", recall )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue>MODELING AND EVALUATION 2</font>\n",
    "\n",
    "**Choose the method you will use for dividing your data into training and testing splits (i.e., are you using Stratified 10-fold cross validation? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. For example, if you are using time series data then you should be using continuous training and testing sets across time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have tried 10 fold cross validation, all  10 round of result are amazingly similar. This is why we do not\n",
    "# believe for this dataset, 10 folder validation is needed. \n",
    "# We've normalized data, that did not make any recognizable model improvement as well. The reason most likely due to\n",
    "# the nature of the dataset are large, only few features are continues input, rest are all hot shot coded already, \n",
    "# data normalization does not play a huge role here. Another reason is we did see from mini lab with the same \n",
    "# dataset, there is no significant important feature in predicting the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "accuracy 0.8281432344450587\n",
      "f1 score 0.6245303274288782\n",
      "precision score 0.6590201076182385\n",
      "recall score 0.5934710533027289\n",
      "confusion matrix\n",
      " [[11156  1204]\n",
      " [ 1594  2327]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.8266077022295928\n",
      "f1 score 0.6244512438472795\n",
      "precision score 0.6526696329254728\n",
      "recall score 0.598571792909972\n",
      "confusion matrix\n",
      " [[11111  1249]\n",
      " [ 1574  2347]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.8288802899084823\n",
      "f1 score 0.631578947368421\n",
      "precision score 0.6558637736885471\n",
      "recall score 0.6090283091048202\n",
      "confusion matrix\n",
      " [[11107  1253]\n",
      " [ 1533  2388]]\n",
      "====Iteration 3  ====\n",
      "accuracy 0.8234752165100424\n",
      "f1 score 0.6213438735177866\n",
      "precision score 0.642681929681112\n",
      "recall score 0.6013771996939556\n",
      "confusion matrix\n",
      " [[11049  1311]\n",
      " [ 1563  2358]]\n",
      "====Iteration 4  ====\n",
      "accuracy 0.8269762299613046\n",
      "f1 score 0.6260454002389485\n",
      "precision score 0.6528239202657807\n",
      "recall score 0.6013771996939556\n",
      "confusion matrix\n",
      " [[11106  1254]\n",
      " [ 1563  2358]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "X = ds.data\n",
    "y = ds.target\n",
    "# normalize the data attributes\n",
    "normalized_X = preprocessing.normalize(X)\n",
    "\n",
    "Ndata=normalized_X\n",
    "labels=y\n",
    "sss = StratifiedShuffleSplit(n_splits=5,test_size = 0.5, train_size=0.5)\n",
    "iter_num=0\n",
    "for train_index, test_index in sss.split(Ndata, labels):\n",
    "    x_train, x_test = Ndata[train_index], Ndata[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "    # train the reusable KNN classifier on the training data       \n",
    "    clf = KNeighborsClassifier(n_neighbors=8, weights='distance', metric='euclidean')\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_hat = clf.predict(x_test) \n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = accuracy_score(y_test,y_hat)\n",
    "    precision = precision_score(y_test, y_hat)\n",
    "    recall = recall_score(y_test, y_hat)\n",
    "    f1=f1_score(y_test, y_hat)\n",
    "    conf = confusion_matrix(y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"f1 score\", f1 )\n",
    "    print(\"precision score\", precision )\n",
    "    print(\"recall score\", recall )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue>MODELING AND EVALUATION 3</font>\n",
    "\n",
    "**Create three different classification/regression models for each task (e.g., random forest, KNN, and SVM for task one and the same or different algorithms for task two). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric. You must investigate different parameters of the algorithms!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigated k value from 2 to 20, that did not make significant enough difference. In some case, resulted\n",
    "# less favorable result. So we keep k=8, large enough to discount the noise, small enough not to waste resources\n",
    "# and still produce same high quality model.\n",
    "# we've also investigated number of stratified 10 fold cross validation.\n",
    "# should we change metric? will computer able to handle it?\n",
    "# should we change weights?\n",
    "# what else?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue>MODELING AND EVALUATION 4</font>\n",
    "\n",
    "**Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue>MODELING AND EVALUATION 5</font>\n",
    "\n",
    "**Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniquesâ€”be sure they are appropriate for your chosen method of validation as discussed in unit 7 of the course.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue>MODELING AND EVALUATION 6</font>\n",
    "\n",
    "**Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue>DEPLOYMENT</font>\n",
    "\n",
    "**How useful is your model for interested parties (i.e., the companies or organizations that might want to use it for prediction)? How would you measure the model's value if it was used by these parties? How would your deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue>EXCEPTIONAL WORK</font>\n",
    "\n",
    "**You have free reign to provide additional analyses. One idea: grid search parameters in a parallelized fashion and visualize the performances across attributes. Which parameters are most significant for making a good model for each classification algorithm?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
