{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red>SMU DS 7331 DATA MINING - MINILAB LOGISTIC REGRESSION AND SVMS</font>\n",
    "\n",
    "**Team Members:**\n",
    "- YuMei Bennett\n",
    "- Liang Huang\n",
    "- Ganesh Kodi\n",
    "- Eric McCandless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue>CREATE MODELS (50 POINTS)</font>\n",
    "\n",
    "**Create a logistic regression model and a support vector machine model for the classification task involved with your dataset. Assess how well each model performs (use 80/20 training/testing split for your data). Adjust parameters of the models to make them more accurate. If your dataset size requires the use of stochastic gradient descent, then linear kernel only is fine to use. That is, the SGDClassifier is fine to use for optimizing logistic regression and linear support vector machines. For many problems, SGD will be required in order to train the SVM model in a reasonable timeframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import all necessary modules\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Imputer\n",
    "import seaborn as sns\n",
    "\n",
    "#Read in dataset.\n",
    "col_names = ['age', 'employ_type', 'pop_num', 'edu_level', 'edu_years', 'marital', 'occ', 'relation', 'race', 'gender', 'cap_gain', 'cap_loss', 'hours_week', 'country_orig', 'income']\n",
    "df = pd.read_csv('adult.csv', names=col_names, header=None)\n",
    "\n",
    "#Replace \"?\" with \"Other_cat\"\n",
    "df['employ_type'] = df['employ_type'].str.replace('?','Other_cat')\n",
    "df['occ'] = df['occ'].str.replace('?','Other_cat')\n",
    "df['country_orig'] = df['country_orig'].str.replace('?','Other_cat')\n",
    "\n",
    "# Binary encoding of the target variable\n",
    "df['income'] = df['income'].apply(lambda inc: 0 if inc ==\" <=50K\" else 1) \n",
    "\n",
    "#Transform employ_type into multiple columns with 0 and 1\n",
    "# use pd.concat to join the new columns with original dataframe then drop the original 'employ_type' column (don't need it anymore)\n",
    "df = pd.concat([df,pd.get_dummies(df['employ_type'], prefix='emp')],axis=1)\n",
    "df.drop(['employ_type'],axis=1, inplace=True)\n",
    "\n",
    "#Transform gender into multiple columns with 0 and 1\n",
    "# use pd.concat to join the new columns with original dataframe then drop the original 'gender' column (don't need it anymore)\n",
    "df = pd.concat([df,pd.get_dummies(df['gender'], prefix='gen')],axis=1)\n",
    "df.drop(['gender'],axis=1, inplace=True)\n",
    "\n",
    "#Transform race into multiple columns with 0 and 1\n",
    "# use pd.concat to join the new columns with original dataframe then drop the original 'race' column (don't need it anymore)\n",
    "df = pd.concat([df,pd.get_dummies(df['race'], prefix='rac')],axis=1)\n",
    "df.drop(['race'],axis=1, inplace=True)\n",
    "\n",
    "#Transform education_level into multiple columns with 0 and 1\n",
    "# use pd.concat to join the new columns with original dataframe then drop the original 'edu_level' column (don't need it anymore)\n",
    "df = pd.concat([df,pd.get_dummies(df['edu_level'], prefix='edu')],axis=1)\n",
    "df.drop(['edu_level'],axis=1, inplace=True)\n",
    "\n",
    "#Consolidate education levels because many of them have the similar impact to target income.\n",
    "df['edu_ SomeCollege'] = df['edu_ Some-college'] + df['edu_ Assoc-acdm'] + df['edu_ Assoc-voc'] \n",
    "df['<HS'] = df['edu_ 12th'] + df['edu_ 11th'] + df['edu_ 10th'] + df['edu_ 9th'] + df['edu_ 7th-8th'] + df['edu_ 5th-6th']+ df['edu_ 1st-4th'] + df['edu_ Preschool'] \n",
    "df=df.drop(['edu_ Some-college','edu_ Assoc-acdm','edu_ Assoc-voc', 'edu_ 12th', 'edu_ 11th','edu_ 10th','edu_ 9th','edu_ 7th-8th','edu_ 7th-8th','edu_ 5th-6th','edu_ 1st-4th','edu_ Preschool'], 1)\n",
    "\n",
    "# drop edu_years as it is highly correlated with edu_level.\n",
    "df=df.drop(['edu_years'], 1)\n",
    "\n",
    "#Transform relation into multiple columns with 0 and 1\n",
    "# use pd.concat to join the new columns with original dataframe then drop the original 'relation' column (don't need it anymore)\n",
    "df = pd.concat([df,pd.get_dummies(df['relation'], prefix='rel')],axis=1)\n",
    "df.drop(['relation'],axis=1, inplace=True)\n",
    "\n",
    "#Transform marital into multiple columns with 0 and 1\n",
    "# use pd.concat to join the new columns with original dataframe then drop the original 'marital' column (don't need it anymore)\n",
    "df = pd.concat([df,pd.get_dummies(df['marital'], prefix='mar')],axis=1)\n",
    "df.drop(['marital'],axis=1, inplace=True)\n",
    "\n",
    "#Consolidate marital status because too many similar categories.  Married-civ-spouse and Married-AF-spouse are similar as are non-married.\n",
    "df['Married'] = df['mar_ Married-civ-spouse'] + df['mar_ Married-AF-spouse'] \n",
    "df['Sep_Div_Absent_Wid'] = df['mar_ Divorced'] + df['mar_ Separated'] + df['mar_ Widowed'] + df['mar_ Married-spouse-absent']\n",
    "df['Never_Married'] = df['mar_ Never-married']\n",
    "df=df.drop(['mar_ Married-civ-spouse','mar_ Married-AF-spouse','mar_ Divorced', 'mar_ Separated', 'mar_ Widowed','mar_ Married-spouse-absent','mar_ Never-married'], 1)\n",
    "\n",
    "#Transform occ into multiple columns with 0 and 1\n",
    "# use pd.concat to join the new columns with original dataframe then drop the original 'occ' column (don't need it anymore)\n",
    "df = pd.concat([df,pd.get_dummies(df['occ'], prefix='occu')],axis=1)\n",
    "df.drop(['occ'],axis=1, inplace=True)\n",
    "\n",
    "#Consolidate occupation by combining 'Other-service', 'Other_cat', and 'Armed-Forces. 'Other' categories are combined because they are not defined and Armed-Forces has an extremely small number of occurences.\n",
    "df['occu_ Other'] = df['occu_ Other-service'] + df['occu_ Other_cat'] + df['occu_ Armed-Forces'] \n",
    "df=df.drop(['occu_ Other-service','occu_ Other_cat','occu_ Armed-Forces'], 1)\n",
    "\n",
    "# drop pop_num as population number is an assigned index number, it has no meaning or contribution to our target income.\n",
    "df=df.drop(['pop_num'], 1)\n",
    "\n",
    "# Combine all non-U.S. native countries as only ~10% people are not from US - code native country into binary 1=United-States\n",
    "df['country_orig'] = df['country_orig'].apply(lambda inc: 1 if inc ==\" United-States\" else 0) \n",
    "\n",
    "# merge capital gain and capital losscap_gain and cap_loss as it can be mathmatically concatenated into a single feature cap_gain_loss = cap_gain - cap_loss.\n",
    "df['cap_gain-loss'] = df['cap_gain'] - df['cap_loss'] \n",
    "df=df.drop(['cap_gain','cap_loss'], 1)\n",
    "df.head(10)\n",
    "\n",
    "#Run logistic and supprt vetor machine model\n",
    "#Use 80/20 training/testing split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'income' in df:\n",
    "    y = df['income'].values # get the labels we want\n",
    "    del df['income'] # get rid of the class label\n",
    "    X = df.values # use everything else to predict!\n",
    "\n",
    "    ## X and y are now numpy matrices, by calling 'values' on the pandas data frames we\n",
    "    #    have converted them into simple matrices to use with scikit learn\n",
    "    \n",
    "    \n",
    "# to use the cross validation object in scikit learn, we need to grab an instance\n",
    "#    of the object and set it up. This object will be able to split our data into \n",
    "#    training and testing splits\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "lr_clf = LogisticRegression(penalty='l2', C=1.0, class_weight=None) # get object\n",
    "\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1\n",
    "    \n",
    "# Also note that every time you run the above code\n",
    "#   it randomly creates a new training and testing set, \n",
    "#   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# plot the correlation matrix using seaborn\n",
    "sns.set(style=\"darkgrid\") # one of the many styles to plot using\n",
    "\n",
    "f, ax = plt.subplots(figsize=(9, 9))\n",
    "\n",
    "sns.heatmap(df.corr(), cmap=cmap, annot=True, ax=ax)\n",
    "\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpret the weights\n",
    "\n",
    "# iterate over the coefficients\n",
    "weights = lr_clf.coef_.T # take transpose to make a column vector\n",
    "variable_names = df.columns\n",
    "for coef, name in zip(weights,variable_names):\n",
    "    print(name, 'has weight of', coef[0])\n",
    "    \n",
    "# does this look correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# we want to normalize the features based upon the mean and standard deviation of each column. \n",
    "# However, we do not want to accidentally use the testing data to find out the mean and std (this would be snooping)\n",
    "# to Make things easier, let's start by just using whatever was last stored in the variables:\n",
    "##    X_train , y_train , X_test, y_test (they were set in a for loop above)\n",
    "\n",
    "# scale attributes by the training set\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train) # find scalings for each column that make this zero mean and unit std\n",
    "# the line of code above only looks at training data to get mean and std and we can use it \n",
    "# to transform new feature data\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) # apply those means and std to the test set (without snooping at the test set values)\n",
    "\n",
    "# train the model just as before\n",
    "lr_clf = LogisticRegression(penalty='l2', C=0.05) # get object, the 'C' value is less (can you guess why??)\n",
    "lr_clf.fit(X_train_scaled,y_train)  # train object\n",
    "\n",
    "y_hat = lr_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort these attributes and spit them out\n",
    "zip_vars = zip(lr_clf.coef_.T,df.columns) # combine attributes\n",
    "zip_vars = sorted(zip_vars)\n",
    "for coef, name in zip_vars:\n",
    "    print(name, 'has weight of', coef[0]) # now print them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's make a pandas Series with the names and values, and plot them\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "weights = pd.Series(lr_clf.coef_[0],index=df.columns)\n",
    "weights.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# we want to normalize the features based upon the mean and standard deviation of each column. \n",
    "# However, we do not want to accidentally use the testing data to find out the mean and std (this would be snooping)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "# you can apply the StandardScaler function inside of the cross-validation loop \n",
    "#  but this requires the use of PipeLines in scikit. \n",
    "#  A pipeline can apply feature pre-processing and data fitting in one compact notation\n",
    "#  Here is an example!\n",
    "\n",
    "std_scl = StandardScaler()\n",
    "lr_clf = LogisticRegression(penalty='l2', C=0.05) \n",
    "\n",
    "# create the pipline\n",
    "piped_object = Pipeline([('scale', std_scl),  # do this\n",
    "                         ('logit_model', lr_clf)]) # and then do this\n",
    "\n",
    "weights = []\n",
    "# run the pipline cross validated\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(X,y)):\n",
    "    piped_object.fit(X[train_indices],y[train_indices])  # train object\n",
    "    # it is a little odd getting trained objects from a  pipeline:\n",
    "    weights.append(piped_object.named_steps['logit_model'].coef_[0])\n",
    "    \n",
    "\n",
    "weights = np.array(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay, so run through the cross validation loop and set the training and testing variable for one single iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets investigate SVMs on the data and play with the parameters and kernels\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# train the model just as before\n",
    "svm_clf = SVC(C=0.5, kernel='rbf', degree=3, gamma='auto') # get object\n",
    "svm_clf.fit(X_train_scaled, y_train)  # train object\n",
    "\n",
    "y_hat = svm_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the support vectors\n",
    "print(svm_clf.support_vectors_.shape)\n",
    "print(svm_clf.support_.shape)\n",
    "print(svm_clf.n_support_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ganesh Updates Starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model just as before\n",
    "svm_clf_lin = SVC(kernel='linear')# Linear Kernel\n",
    "svm_clf_lin.fit(X_train_scaled, y_train)  # train object\n",
    "\n",
    "y_hat_lin = svm_clf_lin.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc_lin = mt.accuracy_score(y_test,y_hat_lin)\n",
    "conf_lin = mt.confusion_matrix(y_test,y_hat_lin)\n",
    "print('accuracy:', acc_lin )\n",
    "print(conf_lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model just as before\n",
    "svm_clf_pol = SVC(kernel='poly')# Polynomial Kernel\n",
    "svm_clf_pol.fit(X_train_scaled, y_train)  # train object\n",
    "\n",
    "y_hat_pol = svm_clf_pol.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc_pol = mt.accuracy_score(y_test,y_hat_pol)\n",
    "conf_pol = mt.confusion_matrix(y_test,y_hat_pol)\n",
    "print('accuracy:', acc_pol )\n",
    "print(conf_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure number\n",
    "fignum = 1\n",
    "\n",
    "# fit the model\n",
    "for kernel in ('linear', 'poly', 'rbf'):\n",
    "    clf = SVC(kernel=kernel, gamma=2)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # plot the line, the points, and the nearest vectors to the plane\n",
    "    plt.figure(fignum, figsize=(4, 3))\n",
    "    plt.clf()\n",
    "\n",
    "    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80,\n",
    "                facecolors='none', zorder=10, edgecolors='k')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.Paired,\n",
    "                edgecolors='k')\n",
    "\n",
    "    plt.axis('tight')\n",
    "    x_min = -3\n",
    "    x_max = 3\n",
    "    y_min = -3\n",
    "    y_max = 3\n",
    "\n",
    "    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n",
    "    Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(XX.shape)\n",
    "    plt.figure(fignum, figsize=(4, 3))\n",
    "    plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\n",
    "    plt.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'],\n",
    "                levels=[-.5, 0, .5])\n",
    "\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    fignum = fignum + 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue>MODEL ADVANTAGES (10 POINTS)</font>\n",
    "\n",
    "**Discuss the advantages of each model for each classification task. Does one type of model offer superior performance over another in terms of prediction accuracy? In terms of training time or efficiency? Explain in detail.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue>INTERPRET FEATURE IMPORTANCE (30 POINTS)</font>\n",
    "\n",
    "**Use the weights from logistic regression to interpret the importance of different features for the classification task. Explain your interpretation in detail. Why do you think some variables are more important?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort these attributes and spit them out\n",
    "zip_vars = zip(lr_clf.coef_.T,df.columns) # combine attributes\n",
    "zip_vars = sorted(zip_vars)\n",
    "for coef, name in zip_vars:\n",
    "    print(name, 'has weight of', coef[0]) # now print them out\n",
    "    \n",
    "# now let's make a pandas Series with the names and values, and plot them\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "weights = pd.Series(lr_clf.coef_[0],index=df.columns)\n",
    "weights.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Based on above weightage and bar graph analysis following features are considered to be highly importants among all other features\n",
    "1. cap_gain-loss --> composite feature of capital gain and loss is taking high precendence  with weight of 1.27. Higher capital person tends to fall into > 55K category.\n",
    "2. Married status of Marital feature --> With weight of 0.44, Married person tends to earn more \n",
    "3. Age --> Age with weight of  0.36, is also another feature which determines the income level \n",
    "4. Education level  '<HS'  --> With negative weight of 0.37, education level lesser than high school adversely affecting the income of individual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue>INTERPRET SUPPORT VECTORS (10 POINTS)</font>\n",
    "\n",
    "**Look at the chosen support vectors for the classification task. Do these provide any insight into the data? Explain. If you used stochastic gradient descent (and therefore did not explicitly solve for support vectors), try subsampling your data to train the SVC model— then analyze the support vectors from the subsampled dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
